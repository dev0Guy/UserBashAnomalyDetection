{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash Script coomand Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Config\n",
    "LABEL_PATH: str = \"./label_data.csv\"\n",
    "USER_FILE_REGEX: str = \"./FraudedRawData/User*\"\n",
    "ATTACK_SAMPLE_RATIO: float = 0.1\n",
    "NORMALIZATION_EPSILON: float = 1e-6\n",
    "DROPOUT: float = 0.1\n",
    "SEGMENT_TOKEN: int = 100\n",
    "EMBEDDING: int = 32\n",
    "BATCH_SIZE: int = 128\n",
    "EPOCHS: int = 5\n",
    "HEADS: int = 5\n",
    "NN_UNIT: int = 64\n",
    "SEED: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "from keras.metrics import AUC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from functional import seq\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import collections\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segment_data() -> pd.DataFrame:\n",
    "    def get_file_conetent(file: Path):\n",
    "        with open(file, \"r\") as f:\n",
    "            return seq(f.readlines()).map(lambda line: line.strip()).to_list()\n",
    "\n",
    "    def split_into_segments(content: list[str], *, segment_size: int = SEGMENT_TOKEN):\n",
    "        return list(chunked(content, segment_size))\n",
    "\n",
    "    def join_segments(content: list[list[str]]):\n",
    "        return seq(content).map(lambda line: \" \".join(line))\n",
    "\n",
    "    files_paths: list[Path] = glob.glob(USER_FILE_REGEX)\n",
    "    files: list[list[str]] = (\n",
    "        seq(files_paths)\n",
    "        .map(get_file_conetent)\n",
    "        .map(split_into_segments)\n",
    "        .map(join_segments)\n",
    "    )\n",
    "    files = seq(files_paths).map(lambda s: s.split(\"/\")[-1]).zip(files).to_dict()\n",
    "    segment_df: pd.DataFrame = pd.DataFrame.from_dict(files).transpose()\n",
    "    segment_df.sort_index(inplace=True)\n",
    "    segment_df = segment_df.reset_index()\n",
    "    segment_df = segment_df.melt(\n",
    "        id_vars=\"index\", var_name=\"SegmentIndex\", value_name=\"SegmentText\"\n",
    "    )\n",
    "    segment_df.rename(columns={\"index\": \"Id\"}, inplace=True)\n",
    "    return segment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels() -> pd.DataFrame:\n",
    "    def extract_index(col_name):\n",
    "        match = re.match(r\"(\\d+)-(\\d+)\", col_name)\n",
    "        if match:\n",
    "            return int(int(match.group(1)) / 100)\n",
    "        return col_name\n",
    "\n",
    "    label_df: pd.DataFrame = pd.read_csv(LABEL_PATH)\n",
    "    label_df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    label_df.index.name = None\n",
    "    label_df.rename(columns=lambda x: extract_index(x), inplace=True)\n",
    "    label_df = label_df.astype(float)\n",
    "    label_df.head()\n",
    "    label_df = label_df.reset_index()\n",
    "    label_df = label_df.melt(\n",
    "        id_vars=\"index\", var_name=\"SegmentIndex\", value_name=\"Label\"\n",
    "    )\n",
    "    label_df.rename(columns={\"index\": \"Id\"}, inplace=True)\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vocab: dict[str, int], text: str):\n",
    "    return np.array([vocab[word] for word in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df: pd.DataFrame) -> dict:\n",
    "    vocab_counter = collections.Counter()\n",
    "\n",
    "    def count_words(text):\n",
    "        tokens = [word for word in text.split(\" \")]\n",
    "        return collections.Counter(tokens)\n",
    "\n",
    "    for segment in df[\"SegmentText\"]:\n",
    "        vocab_counter.update(count_words(segment))\n",
    "    return dict(map(lambda x: (x[1], x[0]), enumerate(vocab_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_train_df(\n",
    "    df: pd.DataFrame, *, id: int, ratio: float = ATTACK_SAMPLE_RATIO\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    user_data = df[df[\"Id\"] == id]\n",
    "    other_users_data = df[df[\"Id\"] != id].sample(int(len(user_data) * ratio))\n",
    "    other_users_data[\"Label\"] = 1\n",
    "    data = pd.concat([user_data, other_users_data], axis=0)\n",
    "    X = np.vstack(data[\"Features\"].values)\n",
    "    labels = np.vstack(data[\"Label\"]).astype(int)\n",
    "    y = np.zeros((labels.shape[0], 2))\n",
    "    y[np.arange(labels.shape[0]), labels.flatten()] = 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_validate_df(df: pd.DataFrame, *, id: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    data = df[df[\"Id\"] == id]\n",
    "    X = np.vstack(data[\"Features\"].values)\n",
    "    labels = np.vstack(data[\"Label\"]).astype(int)\n",
    "    y = np.zeros((labels.shape[0], 2))\n",
    "    y[np.arange(labels.shape[0]), labels.flatten()] = 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Process & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments: pd.DataFrame = load_segment_data()\n",
    "labels: pd.DataFrame = load_labels()\n",
    "df = pd.merge(\n",
    "    segments,\n",
    "    labels,\n",
    "    on=[\"Id\", \"SegmentIndex\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "df[\"Id\"] = df[\"Id\"].str.extract(r\"User(\\d+)\").astype(int)\n",
    "df = df.sort_values(by=[\"SegmentIndex\", \"Id\"], ascending=[True, True])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_embeder: dict[str, int] = create_vocab(df)\n",
    "df[\"Features\"] = df[\"SegmentText\"].apply(lambda x: extract_features(vocab_embeder, x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 5), (1000, 5), (3000, 5))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_label: pd.Series = df[\"Label\"].notna()\n",
    "for_validation: pd.Series = df[\"SegmentIndex\"] >= 50\n",
    "\n",
    "validation_df = df[(has_label) & (for_validation)]\n",
    "train_df = df[(has_label) & (~for_validation)]\n",
    "test_df = df[~has_label]\n",
    "\n",
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=DROPOUT):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=NORMALIZATION_EPSILON)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=NORMALIZATION_EPSILON)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, *, training: bool = True) -> keras.Model:\n",
    "    input = keras.Input(shape=(SEGMENT_TOKEN,))\n",
    "    emb = TokenAndPositionEmbedding(SEGMENT_TOKEN, vocab_size, EMBEDDING)(input)\n",
    "    transformer = TransformerBlock(EMBEDDING, HEADS, NN_UNIT)(emb, training=training)\n",
    "    avg = layers.GlobalAveragePooling1D()(transformer)\n",
    "    out = layers.Dense(2, activation=\"softmax\")(avg)\n",
    "    model = keras.Model(inputs=input, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User-0 ====================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.8364 - auc: 0.8298 - loss: 0.5515\n",
      "==================== User-1 ====================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3091 - auc: 0.1949 - loss: 0.8480\n",
      "==================== User-2 ====================\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x2dcb60fe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.1273 - auc: 0.0605 - loss: 1.4451\n",
      "==================== User-3 ====================\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x3349147c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.2727 - auc: 0.3041 - loss: 0.7442\n",
      "==================== User-4 ====================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.9091 - auc: 0.9045 - loss: 0.4314\n"
     ]
    }
   ],
   "source": [
    "models = collections.defaultdict(lambda: create_model(len(vocab_embeder)))\n",
    "for id in range(5): #df[\"Id\"].unique():\n",
    "    model = models[id]\n",
    "    print(20*\"=\", f\"User-{id}\" ,20*\"=\")\n",
    "    X, y = user_train_df(train_df, id=id)\n",
    "    model.compile(\"adam\", \"CategoricalCrossentropy\", metrics=[\"accuracy\",AUC(name=\"auc\")])\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(arr, n=10):\n",
    "    # Sort the input array indices based on values\n",
    "    sorted_indices = np.argsort(arr)\n",
    "    \n",
    "    # Sort the array based on indices\n",
    "    sorted_arr = arr[sorted_indices]\n",
    "    \n",
    "    # Get the threshold value for the top n segments\n",
    "    threshold = sorted_arr[-n]\n",
    "    \n",
    "    # Create a result array initialized with zeros\n",
    "    res_arr = np.zeros_like(arr)\n",
    "    \n",
    "    # Assign 1 to segments with values >= threshold\n",
    "    res_arr[arr >= threshold] = 1\n",
    "    \n",
    "    return res_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== User-0 ====================\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "tp=0.02, tn=0.82\n",
      "==================== User-1 ====================\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "tp=0.05, tn=0.85\n",
      "==================== User-2 ====================\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "tp=0.07, tn=0.87\n",
      "==================== User-3 ====================\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "tp=0.08, tn=0.88\n",
      "==================== User-4 ====================\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "tp=0.06, tn=0.86\n"
     ]
    }
   ],
   "source": [
    "for id in range(5):\n",
    "    print(20*\"=\", f\"User-{id}\" ,20*\"=\")\n",
    "    model = models[id]\n",
    "    X_test,y_test = user_validate_df(validation_df, id=id)\n",
    "    smooth_pred = smooth(model.predict(X_test)[:, 1])\n",
    "    cm = confusion_matrix(y_test[:, 1], smooth_pred)\n",
    "    tp = cm[1, 1] / y_test.shape[0]  # True positives (correct masqueraded segments)\n",
    "    tn = cm[0, 0] / y_test.shape[0]  # True negatives (correct benign segments)\n",
    "    fp = cm[0, 1] / y_test.shape[0]  # False positives (misclassified as masqueraded)\n",
    "    fn = cm[1, 0] / y_test.shape[0] \n",
    "    print(f\"{tp=}, {tn=}\")\n",
    "\n",
    "    # confusion_matrix\n",
    "    # tp = ((smooth_pred == 1) == (y_test[:,1] == 1)).sum() / y_test.shape[0]\n",
    "    # tn = ((smooth_pred == 0) == (y_test[:,1] == 0)).sum() / y_test.shape[0]\n",
    "    # fp = ((smooth_pred == 0) == (y_test[:,1] == 0)).sum() / y_test.shape[0]\n",
    "    # print(tp)\n",
    "    # print(tn)\n",
    "                         \n",
    "    # score = \n",
    "    # print(f\"Test Loss: {loss:.4f}\")\n",
    "    # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Data is saved inside FraudedRawData. Loading data happend into simple ways. First we load the data inself and create table for each user [Id, SegId ,SegmentText, Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "LABEL_PATH: str = \"./label_data.csv\"\n",
    "USER_FILE_REGEX: str = \"./FraudedRawData/User*\"\n",
    "ATTACK_SAMPLE_RATIO: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>User0</th>\n",
       "      <td>cat nawk nawk uname pwd echo echo ksh uname st...</td>\n",
       "      <td>xgvis ls ls sh sh xgvis sh sh xgvis Sqpe sendm...</td>\n",
       "      <td>uname pwd echo echo ksh ls sendmail movemail m...</td>\n",
       "      <td>mywsh mywsh xset cat nawk nawk uname pwd echo ...</td>\n",
       "      <td>led uname uname pwd echo echo ksh ls ksh ls ls...</td>\n",
       "      <td>ul sh man man col col neqn nroff xwsh ksh move...</td>\n",
       "      <td>sh ls sh sh sh xgvis sh sh xgvis rm sh ls sh s...</td>\n",
       "      <td>sh egrep sed sh sed sh sh sh sed sh sed sh sh ...</td>\n",
       "      <td>help sh less sh less sh less rm sh sh find cat...</td>\n",
       "      <td>rm sh sh find cat sed help sh less rm sh sh sh...</td>\n",
       "      <td>...</td>\n",
       "      <td>sendmail ksh cat more sendmail sendmail sendma...</td>\n",
       "      <td>true grep date lp find tail ls sed FIFO cat ge...</td>\n",
       "      <td>awk cat post rm generic ln ln generic lp sh ge...</td>\n",
       "      <td>sendmail sendmail sendmail sh MediaMai sendmai...</td>\n",
       "      <td>hostname id nawk getopt true true true grep da...</td>\n",
       "      <td>nawk getopt true true grep date lp find expr g...</td>\n",
       "      <td>generic gethost download enscript ksh hostname...</td>\n",
       "      <td>sed FIFO cat generic ls generic cat generic ls...</td>\n",
       "      <td>ls acroread acroread acroread expr cat acrorea...</td>\n",
       "      <td>ksh ksh nawk sendmail deroff sort spell spell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User1</th>\n",
       "      <td>cpp sh xrdb cpp sh xrdb mkpts hostname stty en...</td>\n",
       "      <td>id nawk getopt true true grep date lp find mkd...</td>\n",
       "      <td>find mkdir expr generic cat file ppost awk ppo...</td>\n",
       "      <td>sh MediaMai sendmail emacs-20 ls hostname id n...</td>\n",
       "      <td>generic generic date generic gethost download ...</td>\n",
       "      <td>tcpostio tcpostio tcpostio cat generic ls gene...</td>\n",
       "      <td>id nawk getopt true grep date lp find mkdir ex...</td>\n",
       "      <td>netscape mkpts hostname stty .java_wr expr exp...</td>\n",
       "      <td>expr expr dirname basename egrep egrep egrep e...</td>\n",
       "      <td>egrep egrep egrep expr expr expr dirname java ...</td>\n",
       "      <td>...</td>\n",
       "      <td>ps ps grep ps grep grep ps grep grep ps grep g...</td>\n",
       "      <td>tcsh make tcsh hostname stty fec driver tcsh m...</td>\n",
       "      <td>MediaMai hostname stty hostname stty telnet te...</td>\n",
       "      <td>tcsh make tcsh hostname stty fec driver tcsh m...</td>\n",
       "      <td>make tcsh hostname stty fec be driver tcsh ld_...</td>\n",
       "      <td>tcsh xterm emacs-20 netscape netscape cat mail...</td>\n",
       "      <td>tail ls sed FIFO generic hostname id nawk geto...</td>\n",
       "      <td>netscape netscape hostname id nawk getopt true...</td>\n",
       "      <td>id nawk getopt true true grep date lp find tai...</td>\n",
       "      <td>LOCK true ls sed FIFO cat date generic generic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User10</th>\n",
       "      <td>cpp sh xrdb cpp sh xrdb mkpts hostname env csh...</td>\n",
       "      <td>netscape netscape rlogin rlogin tput movemail ...</td>\n",
       "      <td>tset launchef sh launchef movemail movemail la...</td>\n",
       "      <td>UNLOCK rmdir generic tektroni sh LOCK hostname...</td>\n",
       "      <td>csh virtex virtex virtex virtex virtex virtex ...</td>\n",
       "      <td>awk cat post rm generic ln ln generic lp getpg...</td>\n",
       "      <td>ghostvie hostname id nawk getopt true true tru...</td>\n",
       "      <td>FIFO cat date generic generic date generic dow...</td>\n",
       "      <td>stty tset resize movemail movemail sendmail se...</td>\n",
       "      <td>gzip sh sh hostname id nawk getopt true grep d...</td>\n",
       "      <td>...</td>\n",
       "      <td>more rm ls more ex hostname id nawk getopt tru...</td>\n",
       "      <td>movemail movemail movemail movemail movemail m...</td>\n",
       "      <td>env csh csh csh userenv sh csh kill wait4wm xh...</td>\n",
       "      <td>movemail movemail movemail sendmail sendmail s...</td>\n",
       "      <td>tellwm xprop endsessi xdm 4Dwm toolches xclock...</td>\n",
       "      <td>xdm toolches 4Dwm cpp sh xrdb cpp sh xrdb mkpt...</td>\n",
       "      <td>hostname tset hostname date env tcsh tcsh tcsh...</td>\n",
       "      <td>getopt true true grep date lp find expr generi...</td>\n",
       "      <td>userenv wait4wm xhost xsetroot reaper cat mail...</td>\n",
       "      <td>hostname cat mail csh hostname stty tset rlogi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User11</th>\n",
       "      <td>touch touch cat ls sed ln rm sed ln rm chmod s...</td>\n",
       "      <td>hostname tty arch hostname tset arch stty ksh ...</td>\n",
       "      <td>sh gettxt hostname gettxt gettxt gettxt xconfi...</td>\n",
       "      <td>more more ls ls more ls ls cat more col sh col...</td>\n",
       "      <td>hostname arch cat tset tty stty ksh hostname a...</td>\n",
       "      <td>tty hostname hostname hostname arch arch arch ...</td>\n",
       "      <td>launchef sh sh rm MediaMai launchef launchef s...</td>\n",
       "      <td>endsessi xclock xbiff xclock xclock 4Dwm xcloc...</td>\n",
       "      <td>launchef launchef sh faces launchef launchef s...</td>\n",
       "      <td>.xinitrc hostname tty hostname arch hostname w...</td>\n",
       "      <td>...</td>\n",
       "      <td>rm ksh cat tty hostname arch tset stty ksh lau...</td>\n",
       "      <td>endsessi .xinitrc hostname tty hostname arch h...</td>\n",
       "      <td>MediaMai telnet rm ksh xterm netscape netscape...</td>\n",
       "      <td>netscape netscape launchef launchef sh netstat...</td>\n",
       "      <td>file post awk cat post rm generic ln ln generi...</td>\n",
       "      <td>generic ln ln generic lp getpgrp LOCK true ls ...</td>\n",
       "      <td>whoami .xinitrc cpp sh xrdb cat tty hostname a...</td>\n",
       "      <td>emacs-20 ksh uname nawk cpp cc1 gcc gcc ls a.o...</td>\n",
       "      <td>cpp cc1 as ld_ nm ld gcc gcc a.out emacs-20 un...</td>\n",
       "      <td>cpp cc1 as ld_ nm ld gcc gcc a.out emacs-20 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User12</th>\n",
       "      <td>cpp sh xrdb mkpts test [ stty tset [ uname env...</td>\n",
       "      <td>hostname [ cat [ stty tset [ uname mail mail m...</td>\n",
       "      <td>date lp find mkdir expr generic cat file ppost...</td>\n",
       "      <td>generic generic date generic download gethost ...</td>\n",
       "      <td>xconfirm endsessi tellwm tellwm xprop endsessi...</td>\n",
       "      <td>cat date generic generic date generic download...</td>\n",
       "      <td>ln ln generic lp getpgrp LOCK LOCK generic tcp...</td>\n",
       "      <td>sed FIFO cat date generic generic date generic...</td>\n",
       "      <td>ls sed FIFO rm UNLOCK rmdir generic tcppost sh...</td>\n",
       "      <td>hostname id nawk getopt true grep date lp find...</td>\n",
       "      <td>...</td>\n",
       "      <td>LOCK true ls sed FIFO cat date generic generic...</td>\n",
       "      <td>tcpostio tcpostio tcpostio cat generic ls gene...</td>\n",
       "      <td>[ cat [ stty tset [ uname mail emacs-20 hostna...</td>\n",
       "      <td>netscape netscape netscape netscape netscape n...</td>\n",
       "      <td>rm generic ln ln generic lp sh getpgrp LOCK tr...</td>\n",
       "      <td>sh xrdb mkpts test [ stty tset [ uname env ech...</td>\n",
       "      <td>generic date generic rm ls sed FIFO rm UNLOCK ...</td>\n",
       "      <td>hostname tset hostname date launchef sh launch...</td>\n",
       "      <td>mp cat file post awk cat post rm generic ln ln...</td>\n",
       "      <td>ls sed FIFO cat date generic generic date gene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0    \\\n",
       "User0   cat nawk nawk uname pwd echo echo ksh uname st...   \n",
       "User1   cpp sh xrdb cpp sh xrdb mkpts hostname stty en...   \n",
       "User10  cpp sh xrdb cpp sh xrdb mkpts hostname env csh...   \n",
       "User11  touch touch cat ls sed ln rm sed ln rm chmod s...   \n",
       "User12  cpp sh xrdb mkpts test [ stty tset [ uname env...   \n",
       "\n",
       "                                                      1    \\\n",
       "User0   xgvis ls ls sh sh xgvis sh sh xgvis Sqpe sendm...   \n",
       "User1   id nawk getopt true true grep date lp find mkd...   \n",
       "User10  netscape netscape rlogin rlogin tput movemail ...   \n",
       "User11  hostname tty arch hostname tset arch stty ksh ...   \n",
       "User12  hostname [ cat [ stty tset [ uname mail mail m...   \n",
       "\n",
       "                                                      2    \\\n",
       "User0   uname pwd echo echo ksh ls sendmail movemail m...   \n",
       "User1   find mkdir expr generic cat file ppost awk ppo...   \n",
       "User10  tset launchef sh launchef movemail movemail la...   \n",
       "User11  sh gettxt hostname gettxt gettxt gettxt xconfi...   \n",
       "User12  date lp find mkdir expr generic cat file ppost...   \n",
       "\n",
       "                                                      3    \\\n",
       "User0   mywsh mywsh xset cat nawk nawk uname pwd echo ...   \n",
       "User1   sh MediaMai sendmail emacs-20 ls hostname id n...   \n",
       "User10  UNLOCK rmdir generic tektroni sh LOCK hostname...   \n",
       "User11  more more ls ls more ls ls cat more col sh col...   \n",
       "User12  generic generic date generic download gethost ...   \n",
       "\n",
       "                                                      4    \\\n",
       "User0   led uname uname pwd echo echo ksh ls ksh ls ls...   \n",
       "User1   generic generic date generic gethost download ...   \n",
       "User10  csh virtex virtex virtex virtex virtex virtex ...   \n",
       "User11  hostname arch cat tset tty stty ksh hostname a...   \n",
       "User12  xconfirm endsessi tellwm tellwm xprop endsessi...   \n",
       "\n",
       "                                                      5    \\\n",
       "User0   ul sh man man col col neqn nroff xwsh ksh move...   \n",
       "User1   tcpostio tcpostio tcpostio cat generic ls gene...   \n",
       "User10  awk cat post rm generic ln ln generic lp getpg...   \n",
       "User11  tty hostname hostname hostname arch arch arch ...   \n",
       "User12  cat date generic generic date generic download...   \n",
       "\n",
       "                                                      6    \\\n",
       "User0   sh ls sh sh sh xgvis sh sh xgvis rm sh ls sh s...   \n",
       "User1   id nawk getopt true grep date lp find mkdir ex...   \n",
       "User10  ghostvie hostname id nawk getopt true true tru...   \n",
       "User11  launchef sh sh rm MediaMai launchef launchef s...   \n",
       "User12  ln ln generic lp getpgrp LOCK LOCK generic tcp...   \n",
       "\n",
       "                                                      7    \\\n",
       "User0   sh egrep sed sh sed sh sh sh sed sh sed sh sh ...   \n",
       "User1   netscape mkpts hostname stty .java_wr expr exp...   \n",
       "User10  FIFO cat date generic generic date generic dow...   \n",
       "User11  endsessi xclock xbiff xclock xclock 4Dwm xcloc...   \n",
       "User12  sed FIFO cat date generic generic date generic...   \n",
       "\n",
       "                                                      8    \\\n",
       "User0   help sh less sh less sh less rm sh sh find cat...   \n",
       "User1   expr expr dirname basename egrep egrep egrep e...   \n",
       "User10  stty tset resize movemail movemail sendmail se...   \n",
       "User11  launchef launchef sh faces launchef launchef s...   \n",
       "User12  ls sed FIFO rm UNLOCK rmdir generic tcppost sh...   \n",
       "\n",
       "                                                      9    ...  \\\n",
       "User0   rm sh sh find cat sed help sh less rm sh sh sh...  ...   \n",
       "User1   egrep egrep egrep expr expr expr dirname java ...  ...   \n",
       "User10  gzip sh sh hostname id nawk getopt true grep d...  ...   \n",
       "User11  .xinitrc hostname tty hostname arch hostname w...  ...   \n",
       "User12  hostname id nawk getopt true grep date lp find...  ...   \n",
       "\n",
       "                                                      140  \\\n",
       "User0   sendmail ksh cat more sendmail sendmail sendma...   \n",
       "User1   ps ps grep ps grep grep ps grep grep ps grep g...   \n",
       "User10  more rm ls more ex hostname id nawk getopt tru...   \n",
       "User11  rm ksh cat tty hostname arch tset stty ksh lau...   \n",
       "User12  LOCK true ls sed FIFO cat date generic generic...   \n",
       "\n",
       "                                                      141  \\\n",
       "User0   true grep date lp find tail ls sed FIFO cat ge...   \n",
       "User1   tcsh make tcsh hostname stty fec driver tcsh m...   \n",
       "User10  movemail movemail movemail movemail movemail m...   \n",
       "User11  endsessi .xinitrc hostname tty hostname arch h...   \n",
       "User12  tcpostio tcpostio tcpostio cat generic ls gene...   \n",
       "\n",
       "                                                      142  \\\n",
       "User0   awk cat post rm generic ln ln generic lp sh ge...   \n",
       "User1   MediaMai hostname stty hostname stty telnet te...   \n",
       "User10  env csh csh csh userenv sh csh kill wait4wm xh...   \n",
       "User11  MediaMai telnet rm ksh xterm netscape netscape...   \n",
       "User12  [ cat [ stty tset [ uname mail emacs-20 hostna...   \n",
       "\n",
       "                                                      143  \\\n",
       "User0   sendmail sendmail sendmail sh MediaMai sendmai...   \n",
       "User1   tcsh make tcsh hostname stty fec driver tcsh m...   \n",
       "User10  movemail movemail movemail sendmail sendmail s...   \n",
       "User11  netscape netscape launchef launchef sh netstat...   \n",
       "User12  netscape netscape netscape netscape netscape n...   \n",
       "\n",
       "                                                      144  \\\n",
       "User0   hostname id nawk getopt true true true grep da...   \n",
       "User1   make tcsh hostname stty fec be driver tcsh ld_...   \n",
       "User10  tellwm xprop endsessi xdm 4Dwm toolches xclock...   \n",
       "User11  file post awk cat post rm generic ln ln generi...   \n",
       "User12  rm generic ln ln generic lp sh getpgrp LOCK tr...   \n",
       "\n",
       "                                                      145  \\\n",
       "User0   nawk getopt true true grep date lp find expr g...   \n",
       "User1   tcsh xterm emacs-20 netscape netscape cat mail...   \n",
       "User10  xdm toolches 4Dwm cpp sh xrdb cpp sh xrdb mkpt...   \n",
       "User11  generic ln ln generic lp getpgrp LOCK true ls ...   \n",
       "User12  sh xrdb mkpts test [ stty tset [ uname env ech...   \n",
       "\n",
       "                                                      146  \\\n",
       "User0   generic gethost download enscript ksh hostname...   \n",
       "User1   tail ls sed FIFO generic hostname id nawk geto...   \n",
       "User10  hostname tset hostname date env tcsh tcsh tcsh...   \n",
       "User11  whoami .xinitrc cpp sh xrdb cat tty hostname a...   \n",
       "User12  generic date generic rm ls sed FIFO rm UNLOCK ...   \n",
       "\n",
       "                                                      147  \\\n",
       "User0   sed FIFO cat generic ls generic cat generic ls...   \n",
       "User1   netscape netscape hostname id nawk getopt true...   \n",
       "User10  getopt true true grep date lp find expr generi...   \n",
       "User11  emacs-20 ksh uname nawk cpp cc1 gcc gcc ls a.o...   \n",
       "User12  hostname tset hostname date launchef sh launch...   \n",
       "\n",
       "                                                      148  \\\n",
       "User0   ls acroread acroread acroread expr cat acrorea...   \n",
       "User1   id nawk getopt true true grep date lp find tai...   \n",
       "User10  userenv wait4wm xhost xsetroot reaper cat mail...   \n",
       "User11  cpp cc1 as ld_ nm ld gcc gcc a.out emacs-20 un...   \n",
       "User12  mp cat file post awk cat post rm generic ln ln...   \n",
       "\n",
       "                                                      149  \n",
       "User0   ksh ksh nawk sendmail deroff sort spell spell ...  \n",
       "User1   LOCK true ls sed FIFO cat date generic generic...  \n",
       "User10  hostname cat mail csh hostname stty tset rlogi...  \n",
       "User11  cpp cc1 as ld_ nm ld gcc gcc a.out emacs-20 un...  \n",
       "User12  ls sed FIFO cat date generic generic date gene...  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "from functional import seq\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_file_conetent(file: Path):\n",
    "    with open(file, \"r\") as f:\n",
    "        return seq(f.readlines()).map(lambda line: line.strip()).to_list()\n",
    "\n",
    "\n",
    "def split_into_segments(content: list[str], *, segment_size: int = 100):\n",
    "    return list(chunked(content, segment_size))\n",
    "\n",
    "\n",
    "def join_segments(content: list[list[str]]):\n",
    "    return seq(content).map(lambda line: \" \".join(line))\n",
    "\n",
    "\n",
    "# Find all user Files\n",
    "\n",
    "files_paths: list[Path] = glob.glob(USER_FILE_REGEX)\n",
    "files: list[list[str]] = (\n",
    "    seq(files_paths).map(get_file_conetent).map(split_into_segments).map(join_segments)\n",
    ")\n",
    "files = seq(files_paths).map(lambda s: s.split(\"/\")[-1]).zip(files).to_dict()\n",
    "segment_df: pd.DataFrame = pd.DataFrame.from_dict(files).transpose()\n",
    "segment_df.sort_index(inplace=True)\n",
    "segment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to flattend the df for simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SegmentIndex</th>\n",
       "      <th>SegmentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User0</td>\n",
       "      <td>0</td>\n",
       "      <td>cat nawk nawk uname pwd echo echo ksh uname st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User1</td>\n",
       "      <td>0</td>\n",
       "      <td>cpp sh xrdb cpp sh xrdb mkpts hostname stty en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User10</td>\n",
       "      <td>0</td>\n",
       "      <td>cpp sh xrdb cpp sh xrdb mkpts hostname env csh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User11</td>\n",
       "      <td>0</td>\n",
       "      <td>touch touch cat ls sed ln rm sed ln rm chmod s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User12</td>\n",
       "      <td>0</td>\n",
       "      <td>cpp sh xrdb mkpts test [ stty tset [ uname env...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id SegmentIndex                                        SegmentText\n",
       "0   User0            0  cat nawk nawk uname pwd echo echo ksh uname st...\n",
       "1   User1            0  cpp sh xrdb cpp sh xrdb mkpts hostname stty en...\n",
       "2  User10            0  cpp sh xrdb cpp sh xrdb mkpts hostname env csh...\n",
       "3  User11            0  touch touch cat ls sed ln rm sed ln rm chmod s...\n",
       "4  User12            0  cpp sh xrdb mkpts test [ stty tset [ uname env..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_df = segment_df.reset_index()\n",
    "segment_df = segment_df.melt(\n",
    "    id_vars=\"index\", var_name=\"SegmentIndex\", value_name=\"SegmentText\"\n",
    ")\n",
    "segment_df.rename(columns={\"index\": \"Id\"}, inplace=True)\n",
    "segment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label \n",
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SegmentIndex</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id SegmentIndex  Label\n",
       "0  User0            0    0.0\n",
       "1  User1            0    0.0\n",
       "2  User2            0    0.0\n",
       "3  User3            0    0.0\n",
       "4  User4            0    0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_index(col_name):\n",
    "    match = re.match(r\"(\\d+)-(\\d+)\", col_name)\n",
    "    if match:\n",
    "        return int(int(match.group(1)) / 100)\n",
    "    return col_name\n",
    "\n",
    "\n",
    "label_df: pd.DataFrame = pd.read_csv(LABEL_PATH)\n",
    "label_df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "label_df.index.name = None\n",
    "label_df.rename(columns=lambda x: extract_index(x), inplace=True)\n",
    "label_df = label_df.astype(float)\n",
    "label_df.head()\n",
    "label_df = label_df.reset_index()\n",
    "label_df = label_df.melt(id_vars=\"index\", var_name=\"SegmentIndex\", value_name=\"Label\")\n",
    "label_df.rename(columns={\"index\": \"Id\"}, inplace=True)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "value_name (Label) cannot match an element in the DataFrame columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m label_df \u001b[38;5;241m=\u001b[39m label_df\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m----> 2\u001b[0m label_df \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSegmentIndex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m label_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m label_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Documents/GitHub/UserBashAnomalyDetection/venv/lib/python3.11/site-packages/pandas/core/frame.py:9942\u001b[0m, in \u001b[0;36mDataFrame.melt\u001b[0;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m   9932\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaller\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf.melt(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   9933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelt\u001b[39m(\n\u001b[1;32m   9934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9940\u001b[0m     ignore_index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   9941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m-> 9942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmelt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9943\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/UserBashAnomalyDetection/venv/lib/python3.11/site-packages/pandas/core/reshape/melt.py:54\u001b[0m, in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaller\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd.melt(df, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.melt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmelt\u001b[39m(\n\u001b[1;32m     45\u001b[0m     frame: DataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     ignore_index: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value_name \u001b[38;5;129;01min\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) cannot match an element in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe DataFrame columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m     id_vars \u001b[38;5;241m=\u001b[39m ensure_list_vars(id_vars, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_vars\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     59\u001b[0m     value_vars_was_not_none \u001b[38;5;241m=\u001b[39m value_vars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: value_name (Label) cannot match an element in the DataFrame columns."
     ]
    }
   ],
   "source": [
    "label_df = label_df.reset_index()\n",
    "label_df = label_df.melt(id_vars=\"index\", var_name=\"SegmentIndex\", value_name=\"Label\")\n",
    "label_df.rename(columns={\"index\": \"Id\"}, inplace=True)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    segment_df,\n",
    "    label_df,\n",
    "    on=[\"Id\", \"SegmentIndex\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "df[\"Id\"] = df[\"Id\"].str.extract(r\"User(\\d+)\").astype(int)\n",
    "df = df.sort_values(by=[\"SegmentIndex\", \"Id\"], ascending=[True, True])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "vocab_counter = collections.Counter()\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    tokens = [word for word in text.split(\" \")]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "\n",
    "def convert_to_features(text: str):\n",
    "    return np.array([vocab_indexer[word] for word in text.split(\" \")])\n",
    "\n",
    "\n",
    "for segment in df[\"SegmentText\"]:\n",
    "    vocab_counter.update(count_words(segment))\n",
    "vocab_indexer = dict(map(lambda x: (x[1], x[0]), enumerate(vocab_counter)))\n",
    "df[\"Features\"] = df[\"SegmentText\"].apply(convert_to_features)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide into Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_label: pd.Series = df[\"Label\"].notna()\n",
    "for_validation: pd.Series = df[\"SegmentIndex\"] >= 50\n",
    "\n",
    "validation_df = df[(has_label) & (for_validation)]\n",
    "train_df = df[(has_label) & (~for_validation)]\n",
    "test_df = df[~has_label]\n",
    "\n",
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        attn_output, _ = self.att(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output) if training else attn_output\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output) if training else ffn_output\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(-1)\n",
    "        positions = torch.arange(0, maxlen, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_block(\n",
    "            x.permute(1, 0, 2)\n",
    "        )  # Permute to (seq_len, batch, embed_dim)\n",
    "        x = x.permute(1, 2, 0)  # Permute to (batch, embed_dim, seq_len)\n",
    "        x = self.global_avg_pool(x).squeeze(-1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_train_df(train_df, *, id: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    user_data = train_df[train_df[\"Id\"] == id]\n",
    "    other_users_data = train_df[train_df[\"Id\"] != id].sample(int(len(user_data) * 0.1))\n",
    "    other_users_data[\"Label\"] = 1\n",
    "    data = pd.concat([user_data, other_users_data], axis=0)\n",
    "    X = torch.from_numpy(np.vstack(data[\"Features\"].values))\n",
    "    labels = np.vstack(data[\"Label\"]).astype(int)\n",
    "    y = np.zeros((labels.shape[0], 2))\n",
    "    y[np.arange(labels.shape[0]), labels.flatten()] = 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(id: int, n_epochs: int = 1, batch_size: int = 128):\n",
    "    model = TransformerModel(\n",
    "        maxlen=100, vocab_size=len(vocab_indexer), embed_dim=32, num_heads=2, ff_dim=64\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "    # samplex\n",
    "    user_data = train_df[train_df[\"Id\"] == id]\n",
    "    other_users_data = train_df[train_df[\"Id\"] != id].sample(int(len(user_data) * 0.1))\n",
    "    other_users_data[\"Label\"] = 1\n",
    "    data = pd.concat([user_data, other_users_data], axis=0)\n",
    "    X = torch.from_numpy(np.vstack(data[\"Features\"].values))\n",
    "    labels = np.vstack(data[\"Label\"]).astype(int)\n",
    "    y = np.zeros((labels.shape[0], 2))\n",
    "    y[np.arange(labels.shape[0]), labels.flatten()] = 1\n",
    "    #\n",
    "    dataset = MyDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    #\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        pred_train = []\n",
    "        y_true = []\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        y_true = np.array(y_true)[:, 1]\n",
    "        pred_train = np.array(pred_train)\n",
    "        y_true = np.array(y_true).flatten()\n",
    "        pred_train = np.array(pred_train)\n",
    "    acc = metrics.accuracy_score(y_true, pred_train)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, pred_train)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(f\"User: {id}, Accuracy: {acc}, AUC: {roc_auc}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(40):\n",
    "    train_model(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(10):\n",
    "    train_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_user(df: pd.DataFrame, *, id: int, ratio: float = ATTACK_SAMPLE_RATIO):\n",
    "    model = TransformerModel(\n",
    "        maxlen=100, vocab_size=len(vocab_indexer), embed_dim=32, num_heads=4, ff_dim=64\n",
    "    )\n",
    "    # samplex\n",
    "    user_data = df[df[\"Id\"] == id]\n",
    "    other_users_data = df[df[\"Id\"] != id].sample(int(len(user_data) * ratio))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_user(train_df, id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "from functional import seq\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from toolz import curry\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "LABEL_PATH: Path = \"./label_data.csv\"\n",
    "DATA_REGEX: str = \"./FraudedRawData/User*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_bar_plot(data, title: str):\n",
    "    \"\"\"\n",
    "    Takes a dictionary as input and creates a bar plot.\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): A dictionary where keys are categories and values are numerical data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    keys = list(data.keys())\n",
    "    values = list(data.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(keys, values, color=\"skyblue\")\n",
    "    plt.xlabel(\"Bash Command\")\n",
    "    plt.ylabel(\"Frequence\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_segments(content: list[str], *, segment_size: int = 100):\n",
    "    return list(chunked(content, segment_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_segments(content: list[list[str]]):\n",
    "    return seq(content).map(lambda line: \" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry\n",
    "def binary_label_by_user(user_id: int, df: pd.DataFrame):\n",
    "    cpy = df.copy()\n",
    "    cpy[\"label\"] = cpy[\"userId\"].apply(lambda x: int(x != f\"User{user_id}\"))\n",
    "    return cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry\n",
    "def filter_by_user_id(user_id: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return (df[df[\"userId\"] == f\"User{user_id}\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df: pd.DataFrame, value_name: str) -> pd.DataFrame:\n",
    "    \"\"\"organize as a simple table format\"\"\"\n",
    "    df = df.reset_index()\n",
    "    df = df.melt(id_vars=\"index\", var_name=\"segment\", value_name=value_name)\n",
    "    df.rename(columns={\"index\": \"userId\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_conetent(file: Path):\n",
    "    with open(file, \"r\") as f:\n",
    "        return seq(f.readlines()).map(lambda line: line.strip()).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_df(path: Path) -> pd.DataFrame:\n",
    "    label_df: pd.DataFrame = pd.read_csv(path)\n",
    "    label_df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    label_df.index.name = None\n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(regex: str):\n",
    "    files_paths: list[Path] = glob.glob(regex)\n",
    "    files: list[list[str]] = (\n",
    "        seq(files_paths)\n",
    "        .map(get_file_conetent)\n",
    "        .map(split_into_segments)\n",
    "        .map(join_segments)\n",
    "    )\n",
    "    files = seq(files_paths).map(lambda s: s.split(\"/\")[-1]).zip(files).to_dict()\n",
    "    df: pd.DataFrame = pd.DataFrame.from_dict(files).transpose()\n",
    "    new_column_names = {i: f\"{i*100}-{(i+1)*100}\" for i in df.columns}\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df: pd.DataFrame, *, models: dict[int, Pipeline]) -> dict[int, Pipeline]:\n",
    "    for user in models.keys():\n",
    "        tmp_df = binary_label_by_user(user)(df)\n",
    "        models[user].fit(tmp_df[\"text\"], tmp_df[\"label\"])\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df: pd.DataFrame, *, models: dict[int, Pipeline]) -> pd.DataFrame:\n",
    "    results = defaultdict(list)\n",
    "    for user in models.keys():\n",
    "        tmp_df = binary_label_by_user(user)(df)\n",
    "        y_pred = models[user].predict(tmp_df[\"text\"])\n",
    "        y_label = tmp_df[\"label\"]\n",
    "        results[\"preecision\"].append(precision_score(y_label, y_pred, average=\"binary\"))\n",
    "        results[\"recall\"].append(recall_score(y_label, y_pred, average=\"binary\"))\n",
    "        results[\"acc\"].append(accuracy_score(y_label, y_pred))\n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(df: pd.DataFrame, *, models: dict[int, Pipeline]) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for user in models.keys():\n",
    "        filter_df = filter_by_user_id(user)(df)\n",
    "        if not filter_df.empty:\n",
    "            tmp_df = binary_label_by_user(user)(filter_df)\n",
    "            tmp_df[\"label\"] = models[user].predict(tmp_df[\"text\"])\n",
    "            results.append(tmp_df)\n",
    "    return pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipline() -> Pipeline:\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"features\",\n",
    "                TfidfVectorizer(\n",
    "                    sublinear_tf=True,\n",
    "                    analyzer=\"word\",\n",
    "                    ngram_range=(2, 2),\n",
    "                    token_pattern=r\"\\S+\",\n",
    "                    norm=\"l2\",\n",
    "                    min_df=0.0,\n",
    "                    smooth_idf=False,\n",
    "                    max_features=1000,\n",
    "                ),\n",
    "            ),\n",
    "            (\"model\", RandomForestClassifier()),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = load_label_df(LABEL_PATH)\n",
    "text_df = load_text_data(DATA_REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.merge(\n",
    "    flatten_df(text_df, value_name=\"text\"),\n",
    "    flatten_df(label_df, value_name=\"label\"),\n",
    "    on=[\"userId\", \"segment\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "combined[\"segmentIndex\"] = combined[\"segment\"].apply(\n",
    "    lambda s: int(int(s.split(\"-\")[0]) / 100)\n",
    ")\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_label: pd.Series = combined[\"label\"].notna()\n",
    "for_validation: pd.Series = combined[\"segmentIndex\"] >= 50\n",
    "#\n",
    "validation_df = combined[(has_label) & (for_validation)]\n",
    "train_df = combined[(has_label) & (~for_validation)]\n",
    "test_df = combined[~has_label]\n",
    "\n",
    "print(train_df.shape, validation_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atack_sample: pd.DataFrame = validation_df[\n",
    "    (validation_df[\"label\"] == 1.0) & (validation_df[\"userId\"] == \"User0\")\n",
    "]\n",
    "benign_sample: pd.DataFrame = validation_df[\n",
    "    (validation_df[\"label\"] == 0) & (validation_df[\"userId\"] == \"User0\")\n",
    "]\n",
    "\n",
    "atack_sample = atack_sample[\"text\"].apply(lambda s: Counter(s.split(\" \"))).to_list()\n",
    "benign_sample = benign_sample[\"text\"].apply(lambda s: Counter(s.split(\" \"))).to_list()\n",
    "\n",
    "for i in range(3):\n",
    "    dict_to_bar_plot(atack_sample[i], title=\"Attack Segment\")\n",
    "    dict_to_bar_plot(benign_sample[i], title=\"Benign Segment\")\n",
    "    print(\"---\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users: int = len(combined[\"userId\"].unique())\n",
    "models = {user: create_pipline() for user in range(n_users)}\n",
    "models = train(train_df, models=models)\n",
    "test(validation_df, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with all of the data\n",
    "models = train(pd.concat([validation_df, train_df], axis=0), models=models)\n",
    "pred_df = prediction(test_df, models=models)\n",
    "# restrcture for label excel\n",
    "final_df = pd.concat([train_df, validation_df, pred_df], axis=0)[\n",
    "    [\"userId\", \"segment\", \"label\"]\n",
    "].pivot(index=\"userId\", columns=\"segment\", values=\"label\")\n",
    "# save it\n",
    "final_df.to_csv(\"./challengeToFill.csv\")\n",
    "final_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
