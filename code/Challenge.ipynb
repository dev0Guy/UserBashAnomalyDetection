{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bHjQPA3p8v0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.metrics import AUC\n",
        "from collections import Counter\n",
        "from tensorflow import keras\n",
        "from sklearn import metrics\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_FOLDER: Path = \"FraudedRawData\"\n",
        "LABEL_FILE: Path = \"challengeToFill.csv\"\n",
        "DROPOUT_P: float = 1e-1\n",
        "NORMALIZATION_EPSILON: float = 1e-6\n",
        "EMBED_DIM: int = 32\n",
        "SEGMENT_TOKENS_N: int = 100\n",
        "TRANSFORMER_HEADS: int = 5\n",
        "NN_UNITS: int = 64\n",
        "TRAIN_SEGMETN_SIZE: int = 50\n",
        "TEST_SEGMETN_SIZE: int = 100\n",
        "TRAIN_TOKEN_N: int = SEGMENT_TOKENS_N * TRAIN_SEGMETN_SIZE\n",
        "TEST_TOKEN_N: int = SEGMENT_TOKENS_N * TEST_SEGMETN_SIZE\n",
        "MALISIOS_RATIO: float = 0.1\n",
        "USER_FOR_TRAIN: int = 10\n",
        "SEED: int = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_segments(path: Path) -> tuple[dict, list, Counter]:\n",
        "    users = {}\n",
        "    sequences = []\n",
        "    vocab = Counter()\n",
        "\n",
        "    for person_file in os.listdir(path):\n",
        "        if person_file.startswith(\"User\"):\n",
        "            user_file_path = os.path.join(path, person_file)\n",
        "            with open(user_file_path, 'r') as file:\n",
        "                tokens = file.read().split()\n",
        "\n",
        "            user_vocabulary = Counter(tokens)\n",
        "            vocab.update(user_vocabulary)\n",
        "\n",
        "            train_tokens = tokens[:TRAIN_TOKEN_N]\n",
        "            train_segments = [train_tokens[j:j + SEGMENT_TOKENS_N] for j in range(0, len(train_tokens) - SEGMENT_TOKENS_N + 1, 1)]\n",
        "            sequences.extend(train_segments)\n",
        "\n",
        "            test_tokens = tokens[TRAIN_TOKEN_N:]\n",
        "            test_segments = [test_tokens[j:j + SEGMENT_TOKENS_N] for j in range(0, len(test_tokens), SEGMENT_TOKENS_N)]\n",
        "            sequences.extend(test_segments)\n",
        "\n",
        "            user_id = int(person_file[len(\"User\"):])\n",
        "            users[user_id] = (train_segments, test_segments)\n",
        "\n",
        "    return users, sequences, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_labels(users_size: int, path: Path) -> np.ndarray:\n",
        "    all_data_size = TRAIN_SEGMETN_SIZE + TEST_SEGMETN_SIZE\n",
        "    labels = np.zeros((users_size, all_data_size))\n",
        "    with open(path, \"r\") as csvfile:\n",
        "        df = pd.read_csv(csvfile)\n",
        "        for user_id in range(users_size):\n",
        "            if user_id < USER_FOR_TRAIN:\n",
        "                labels[user_id, :] = df.iloc[user_id, 1:].to_numpy(dtype=int)\n",
        "        return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users, sequences, vocab = load_segments(DATA_FOLDER)\n",
        "labels = load_labels(len(users), LABEL_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=NORMALIZATION_EPSILON)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=NORMALIZATION_EPSILON)\n",
        "        self.dropout1 = layers.Dropout(DROPOUT_P)\n",
        "        self.dropout2 = layers.Dropout(DROPOUT_P)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(vocab_size, *, training: bool = True) -> keras.Model:\n",
        "    input = keras.Input(shape=(SEGMENT_TOKENS_N,))\n",
        "    emb = TokenAndPositionEmbedding(SEGMENT_TOKENS_N, vocab_size, EMBED_DIM)(input)\n",
        "    transformer = TransformerBlock(EMBED_DIM, TRANSFORMER_HEADS, NN_UNITS)(\n",
        "        emb, training=training\n",
        "    )\n",
        "    avg = layers.GlobalAveragePooling1D()(transformer)\n",
        "    out = layers.Dense(2, activation=\"softmax\")(avg)\n",
        "    model = keras.Model(inputs=input, outputs=out)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_index = {word:idx for idx,word in enumerate(vocab)}\n",
        "vocab_size = len(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def features(segment):\n",
        "  return [word_index[command] for command in segment]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_negative_samples(data: dict, _id: int, ratio: float = MALISIOS_RATIO, n_others: int = 10) -> tuple[np.ndarray, np.ndarray]:\n",
        "  X: list = []\n",
        "  y: list = []\n",
        "  n_fake_samples = round(len(data[_id][\"X\"]) * ratio)\n",
        "  labeld_one_idx = np.hstack([np.arange(_id),np.arange(_id+1,len(data))])\n",
        "  uid_samples = np.random.choice(labeld_one_idx,n_others,replace=False)\n",
        "\n",
        "  for i, uid in enumerate(uid_samples):\n",
        "    for sid in range(n_fake_samples):\n",
        "      X.append(data[uid][\"X\"][sid])\n",
        "      y.append([0,1])\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_train_test_set(users: dict, labels: np.ndarray, ratio: float = MALISIOS_RATIO, n_others: int = 10):\n",
        "  users_data = {}\n",
        "\n",
        "  for user_id in range(len(users)):\n",
        "    users_data[user_id] = {\"X\":[],\"y\":[],\"X_test\":[],\"y_test\":[]}\n",
        "    train_segments,test_segments = users[user_id]\n",
        "    for seg_id in range(len(train_segments)):\n",
        "      segment = train_segments[seg_id]\n",
        "      sample = features(segment)\n",
        "      users_data[user_id][\"X\"].append(sample)\n",
        "      users_data[user_id][\"y\"].append([1,0])\n",
        "      \n",
        "    for seg_id in range(len(test_segments)):\n",
        "      segment = test_segments[seg_id]\n",
        "      sample = features(segment)\n",
        "      label = labels[user_id][TRAIN_SEGMETN_SIZE + seg_id]\n",
        "      users_data[user_id][\"X_test\"].append(sample)\n",
        "      users_data[user_id][\"y_test\"].append(label)\n",
        "\n",
        "  for user_id in range(len(users_data)):\n",
        "    X_fake, y_fake = generate_negative_samples(users_data, user_id, ratio, n_others)\n",
        "    users_data[user_id][\"X\"] += X_fake\n",
        "    users_data[user_id][\"y\"] += y_fake\n",
        "    users_data[user_id][\"X\"] = np.asarray(users_data[user_id][\"X\"])\n",
        "    users_data[user_id][\"y\"] = np.asarray(users_data[user_id][\"y\"])\n",
        "    users_data[user_id][\"X_test\"] = np.asarray(users_data[user_id][\"X_test\"])\n",
        "    users_data[user_id][\"y_test\"] = np.asarray(users_data[user_id][\"y_test\"])\n",
        "\n",
        "  return users_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "users_data = create_train_test_set(users, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for user_id in range(40):\n",
        "    model = create_model(vocab_size)\n",
        "    model.compile(\"adam\", \"CategoricalCrossentropy\", metrics=[\"accuracy\",AUC(name=\"auc\")])\n",
        "    users_data[user_id][\"Model\"] = model\n",
        "    # fit\n",
        "    users_data[user_id][\"Model\"].fit(users_data[user_id][\"X\"],users_data[user_id][\"y\"],batch_size=128, epochs=1)\n",
        "    # predict probs\n",
        "    users_data[user_id][\"pred_test_prob\"] = users_data[user_id][\"Model\"].predict(users_data[user_id][\"X_test\"])\n",
        "    users_data[user_id][\"pred_train_prob\"] = users_data[user_id][\"Model\"].predict(users_data[user_id][\"X\"])\n",
        "    # convert to classes\n",
        "    users_data[user_id][\"pred_test\"] = np.argmax(users_data[user_id][\"pred_test_prob\"], axis=-1)\n",
        "    users_data[user_id][\"pred_train\"] = np.argmax(users_data[user_id][\"pred_train_prob\"], axis=-1)\n",
        "    # eval on all training samples for final metrics score\n",
        "    acc = metrics.accuracy_score(users_data[user_id][\"y\"][:,1],users_data[user_id][\"pred_train\"])\n",
        "    fpr, tpr, threshold = metrics.roc_curve(users_data[user_id][\"y\"][:,1], users_data[user_id][\"pred_train\"])\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    print(f\"metrics[Accuracy={round(acc,3)}, AUC={round(roc_auc,3)}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth(arr, n=10):\n",
        "  sorted_indices = np.argsort(arr)\n",
        "  sorted_arr = arr[sorted_indices]\n",
        "  smoothed_arr = arr.copy()\n",
        "  threshold_value = sorted_arr[-n] if n <= len(arr) else sorted_arr[0]\n",
        "  smoothed_arr[arr < threshold_value] = 0\n",
        "  smoothed_arr[arr >= threshold_value] = 1\n",
        "  return smoothed_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_and_validation(users_data):\n",
        "    max_score_per_user = (\n",
        "        90 * 1 + 10 * 9\n",
        "    )\n",
        "    max_score = max_score_per_user * 10\n",
        "    score_normalizer = 100 \n",
        "    result = np.zeros((30, SEGMENT_TOKENS_N), dtype=int)\n",
        "    test_score = 0\n",
        "    test_score_smooth = 0\n",
        "\n",
        "    res_test_set = np.zeros((10, SEGMENT_TOKENS_N),  dtype=int)\n",
        "\n",
        "    progress_bar = tqdm(range(40), desc=\"Evaluating Users\")\n",
        "\n",
        "    for user_id in progress_bar:\n",
        "        if user_id < 10:\n",
        "            smooth_pred = smooth(users_data[user_id][\"pred_test_prob\"][:, 1])\n",
        "\n",
        "            correct_predictions = (\n",
        "                users_data[user_id][\"pred_test\"] == users_data[user_id][\"y_test\"]\n",
        "            )\n",
        "\n",
        "            user_score = (\n",
        "                correct_predictions & (users_data[user_id][\"y_test\"] == 1)\n",
        "            ).sum() * 9 + (\n",
        "                correct_predictions & (users_data[user_id][\"y_test\"] == 0)\n",
        "            ).sum()\n",
        "            malicious_found = (\n",
        "                correct_predictions & (users_data[user_id][\"y_test\"] == 1)\n",
        "            ).sum()\n",
        "            benign_found = (\n",
        "                correct_predictions & (users_data[user_id][\"y_test\"] == 0)\n",
        "            ).sum()\n",
        "\n",
        "            smooth_correct_predictions = smooth_pred == users_data[user_id][\"y_test\"]\n",
        "            user_score_smooth = (\n",
        "                smooth_correct_predictions & (users_data[user_id][\"y_test\"] == 1)\n",
        "            ).sum() * 9 + (\n",
        "                smooth_correct_predictions & (users_data[user_id][\"y_test\"] == 0)\n",
        "            ).sum()\n",
        "\n",
        "            acc = metrics.accuracy_score(\n",
        "                users_data[user_id][\"y_test\"], users_data[user_id][\"pred_test\"]\n",
        "            )\n",
        "            fpr, tpr, _ = metrics.roc_curve(\n",
        "                users_data[user_id][\"y_test\"], users_data[user_id][\"pred_test\"]\n",
        "            )\n",
        "            roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "            test_score += user_score\n",
        "            test_score_smooth += user_score_smooth\n",
        "            res_test_set[user_id,] = users_data[user_id][\"pred_test\"].reshape(-1)\n",
        "            # Update progress bar with current user metrics\n",
        "            data = {\n",
        "                    \"User\": user_id,\n",
        "                    \"Accuracy\": round(acc, 3),\n",
        "                    \"AUC\": round(roc_auc, 3),\n",
        "                    \"Score\": user_score,\n",
        "                    \"SmoothScore\": user_score_smooth,\n",
        "                    \"maliciousFound\": f\"{malicious_found * 10}%\",\n",
        "                    \"BenignFound\": f\"{benign_found}%\",\n",
        "                }\n",
        "            progress_bar.set_postfix(data)\n",
        "            print(data)\n",
        "\n",
        "        else:\n",
        "            n_fakes = users_data[user_id][\"pred_test\"].sum()\n",
        "            result[user_id - 10,] = users_data[user_id][\"pred_test\"].reshape(-1)\n",
        "            # Update progress bar with current user metrics\n",
        "            data = {\"User\": user_id, \"Fakes Found\": n_fakes}\n",
        "            progress_bar.set_postfix(data)\n",
        "            print(data)\n",
        "\n",
        "    # Summary print statements\n",
        "    print(\n",
        "        f\"Train Score:{round(test_score / max_score, 3)}% | Normalized = { min(1, round(test_score / score_normalizer, 3))}%\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Test Fakes:{round(res_test_set.sum() / 100, 3)}%)\"\n",
        "    )\n",
        "    return np.asarray(result, dtype=int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = test_and_validation(users_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_df: pd.DataFrame = pd.read_csv(LABEL_FILE)\n",
        "label_df.iloc[10:, 51:] = result\n",
        "label_df.iloc[:, 1:] = label_df.iloc[:, 1:].astype(int)\n",
        "label_df.to_csv(LABEL_FILE)\n",
        "label_df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oXSe66tCmJRr",
        "kS1Jpddkp5SC",
        "7bHjQPA3p8v0",
        "GQbBdBKTq-3r",
        "0BbBfssqwg6q",
        "pPWIjONkmG1B",
        "hSS3xd9_ls97",
        "4Eq2Vdjnlvi6",
        "N43JpUsMl03H"
      ],
      "name": "MFDCA_Challenge_per_user_transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
